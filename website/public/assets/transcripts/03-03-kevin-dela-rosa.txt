      >> You don't have your horse hat on yet.
      >> No.  The horse JS is just here.  You know, it just appears.  Awesome talk.  So I actually really want to know about these server farms under Seattle.  You mentioned those.  Can you tell us a little bit -- have you gone to go see some of these under ground server areas?
      >> Under ground?  They're not under ground, they're in the 7th floor of the Weston downtown, and that's where a lot of the transpacific under sea cables come in.  There's, like, two big Internet exchange points.  But the Weston is cool because it's still independent, so it's not owned by any one company.  And so you get a bunch of cool stuff that happens there.
      >> That's super interesting.  I'm always interesting in areas that you aren't really allowed to go.  But it sounds like you have special access.
      >> I haven't been there.  I just have a wire plugged in there.
      >> Well, maybe you'll get access at some point.
      >> Maybe.
      >> Well, great job.  Great talk.
      [clapping]
      >> So next we have Kevin Dela Rosa here to talk to us.  And my brain just shut off.  Which is kind of convenient.  I actually brought horse JS out here because he has the word intelligence in his title.  And, you know, horse JS just breathes intelligence.  So take it away.
      "Adding intelligence to your JS applications."
      By: Kevin Dela Rosa.
      >> All right.  Thank you.
      All right.  So there's a lot of data out there.  In recent years we've seen an explosion of machine learning applications, deployed in production which leverages this data to do a lot of powerful things.  So there are algorithms out there that help you modify your musical experience, people have been trying to predict age and gender.  And these types of algorithms have even been used to help manage the temperature of your determine stat.
      So in this talk I'm going to give a brief, like, introduction to machine learning and motivated by a few examples along the way and how you can do it in JavaScript.
      So what exactly is machine learning?  So machine learning is a subfield of AI that focuses on algorithms that learn from experience.  And they take.  For example, these algorithms can adapt to your users to provide a more personalized experience.  They can recognize patterns and replace humans in monotonous tasks and separate a signal from noise in large data sets that you might not see otherwise.
      Generally speaking, machine learning applications fall under, like, buckets.  So there are classification algorithms which are used to, like, categorize what type of thing something is.  So, for example, like, spam filtering or nudity detection fall thunder umbrella.  There's also regression problems where you're trying to focus on the task of some continuous number.  So, for example, you might be interesting in finding the stock price of something or predict the age of someone or how much money this movie might gross.  And then there's problems that fall under declustering side of things.  Which is a task that looks to, like, cluster or group different items in a unsupervised manner.
      So think of image where you might be interested in figuring out what pixels correspond to the border or some recommendation algorithms use this kind of abstraction.  And there are many other types of algorithms out there and new ones come up every day.  But typically when you're talking about machine learning in production, you have some pipeline that kind of look like this.  You have a step where you collect data and sanitize it.  You make hypotheses and formulate some machine learning task.  Redefine what features are relevant to your specific application.  Then you train this algorithm by throwing different examples that you have at it.  To tune to different parameters in your learner.
      Then you do some evaluations of your different models to see what has been marked out best.  And when you're satisfied, you deploy it and apply to new data that you see in the real world.
      So I'll walk through a specific example and show how each of these steps apply to it and machine learning.
      Specifically we'll talk through how can you determine, like, the polarity in a tweet; right?  So first off, like, why would you care?  Why would you do this?
      Well, like, if you're an ambassador or somebody in social media, you might want to know, like, how do people perceive a topic or conversation?  And doing these kind of analysis can help you get some insight on how people are talking about your product or who's talking about your product in positive or negative ways.
      So first off we need data; right?  So there are many data sets out there that can be used off the shelf like -- and I've listed some here, Twitter sentiment data set, and a few other data sets targeted at specific topics.  If you don't have a data set that's relevant to you, you might want to scrape Twitter or query, there are APIs to find more relevant tweets to your problems.
      And then there's the caveat things these things don't have labels, we need to have the algorithm learn, so if you don't have an example, you can kick it on of to some crowd so you went program like Mechanical Turk.  It's a crowd sourcing program in Amazon, like, judge this tweet for sentiment and give me back the results so that I can use to train an algorithm or something.
      So for this example, we'll just use something off the shelf.  We'll use the Sanford data set.  So in this example they have 1.6 million tweets collected using query APIs from key words.  And they did something unique in this data set.  They inferred what they thought the sentiment was based on the presence of emoticons.  So a lot of smiley faces, they thought that, like, is a indicator of positive tweets, frowny faces is a indicator of negative tweets.  And this actually works surprisingly well for their task.
      So, yeah, this is, like, a huge CSV with six columns.  Some label for your polarity either positive, negative, or neutral.  Some metadata, and the user, and also the actual tweet text itself with these emoticons out so you don't just learn and check in the presence of the emoticon.
      So, you know, now that we have data, let's move on to how we would actually use it.  So, like, this problem largely falls under the classification family of algorithms.  So you want to predict polarity, meaning you want to predict the class of negative sentiment or positive sentiment.  And in our data set, we have, like, some met data and tweets.  But, like, how do we feed this into an algorithm?  So we need to define some sort of feature set.  So features are how you represent your examples as input into a machine learning algorithm.  Like, at a high level, these are arrays or dictionary that you pass into an algorithm along with something that it's trying to learn in natural language processing, you'll typically see a lot of tasks be accomplished with simple features like bag of words meaning taking a sentence or document, tokenizing it, and then using the result as word features into an algorithm.  So if you want to do a task like that, there are a few different libraries that can accomplish it.  So there's an N PM package called natural that can provide a few different word tokenizers.  This one here is a pretty something one.  All it does is split up your input text based on white space and, like, different punctuation.  And when you provide a sentence, it will spit it out as an array of strings.  In this case it turns it into nine tokens.
      That's nice but, like, tweets are a little bit different than classic text; right?  They're short, full of jargon, they have a lot of abbreviation and informal language.  And the punctuation is oftentimes used to code certain things.  So using a standard tokenizer is probably not what you want to do for this task.
      But based on domain knowledge, what kind of model will feed into our featured algorithm.  On the bag word model but with a few different normalization applied to different tokens.  So first on of we'll create a few equivalence classes.  So we don't care about the users, we just care that there's a user name in this tweet.  Likewise we don't care about what URL is in here.  And in Twitter, or social media there's a lot of, like, use of excessive letters, like, you might say "no" with 50 different Os.  The 50Os and the three Os with the same type of token.
      And as I mentioned, normally tokenization typically doesn't work well in Twitter, so we use a special library.  Called tokenize.  It works good to the natural word tokenizer, you feed it into a sentence or tweet, and it will return different tokens.  As you can see it maintains a lot of punctuation in it and keeps the URL intact and there's a little music symbol, for example.  So this is very helpful for tweets.  So putting it all together, we take in -- here's a random tweet that I found about Comic-Con.  I ran it through the -- through these few steps, and we get out a feature right here, which is the normalized lower case version of this tweet with some of the terms being replaced of the equivalence classes, like, query term and URL.  And let long "yes," like, compressed to a smaller "yes."
      So now that we have a high idea of how we're going to representing relevant represent the examples, let's throw it at an algorithm; right?  But which one do I choose?  There's hundreds of algorithms out there, and I'm just going to list some of the more popular ones up here, like, containers neighbors or sport vector machines, et cetera.
      The rule of thumb here is, like, I tend to use things that are simple and things that I am familiar with first.  And then branch out to more complicated and other things, you know, after they haven't, you know -- weren't performent enough.  So, yeah, just start with your favorite learner.  So we'll use two of my favorites.  And there's an N PM package called brain, which provides an implementation of neuro-networks.  And in this example we have the creation of this net, which is your brain object.  And then you train it by providing different inputs and corresponding outputs, and it tries to learn -- at the bottom you see the run method on a example, and it's trying to learn what value should I output?  So in this example let me add some color to it.  We're trying to learn given some RGB color as a background.  What, like, text color would people most likely want in the foreground?  And this is based on different votes people gave, for example, someone voted to put black text on the screen, for the black background someone voted white, and the purple someone voted white as well.  And as you feed more examples into it, you can better train your algorithm to be more accurate.  But throwing in Orange here will actually output white as the most likely thing that people would want as a foreground color and black as the less likely output.  And the simple, which you can look up the details on Wikipedia.  Natural provides a natural language focus implementation of this classifier.  So what we're doing here is we're throwing different sentences and trying to determine are we interesting in buying something or selling something based on this sentence?
      So, for example, we have I am long Q and short gold, sell gold, things like that.  Like, as you mentioned you collect this data, you train it by calling train.  And then when you're ready to do actual inferences, you can call the clarify method to make predictions on unseen data.  So, like, when you call it I am a short silver, it will say sell; right?
      And this does a lot of featurization for you, it does stop words things that are uninformative like the or of, and your kind of normalization featurization, you can feed that in as an argument instead of text.  So for us we're interesting in passing features that we already set up.  So you can pass an array, and it will do that and won't do any additional normalization.
      Another algorithm that is preside is logistic regression.  And this is the actual algorithm that we'll use to train our tweets sentiment.
      All right.  So now that you have a trained model, like, it's time to see how well it performs.  Like, unless you're, like, super lucky or clairvoyant or something, you're probably going to need to tweak your model and retrain it until you reach some level goodness.
      For the task of classification, these are some of the commonly used metrics, such as classify accuracy, and retrieval specific measures as well as F measures, which are just a combination of precision and recall to give you an overall picture.
      As a recap of our classifier, we have some word regression with some special normalization, we have our data set consisting of about 1.6 million tweets, and I segmented that to 90 tweets used for training and about 10 percent or 160,000 for valuation.  So how does this kind of model do?
      Got an overall accuracy of 80 percent, and is pretty simple for data that is more or less automatically retrieved.
      Depending on your task, you might want to get higher accuracy or, like, have more variation.  So I'll leave the evaluation phase to you honestly.  But to make use of your classifier, after you get some desired result, you're probably going to want to put in production.  So the general steps around this involve saving or exporting the classifier in some application, and creating into an existing location, loading this model or new and unseen data.  So moving forward with our logistic regression example.  Natural after you train by sending through all these tweets.  You can call a save method, which will save the parameters relevant to your algorithm in a JSON file and later on when you're ready to do predictions, you can load this up again without retraining all over and then make predictions on new tweets that come through.
      All right.  So a couple notes on, like, deploying an algorithm like this.  Depending on what your domain or application is, you might be very sensitive to false positives or false negatives and the classification.  So common ways people get around this is depending on how sensitive you are, you may want to specify a complex threshold to where you would fall back if you're not certain or you might put a human on the loop in cases where you're really not certain.
      And other considerations that not out of these models perform the same.  Some require a lot of memory, some are slow in inference or training.  And as part of your evaluation process, you should definitely consider profiling the system as part of the criteria for selecting your algorithm or model.
      And another thing that I oftentimes see people forget is that the real world changes, like, and you want to adapt to that; right?  Like, over time new products will come out, new songs will come out.  People say things differently or use different words.  So you want to, like, complete this feedback loop and, like, retrain, like, often or regularly, depending on your domain.
      So, you know, now that we have the basics mechanics of machine learning and an overview of how it works end to end.  Let's see what we can use our tools on.  Like, I did, like, literature search to see what examples some people did in machine learning and JavaScript.  And I think this is the best example I could find.  Cat face detection.
      This woman wrote a whole on cat faces and pictures.  What she does is cut up images in subwindows and asks the question is there a cat in this subimage?  And if so, there's a cat there, and we draw a box around the images.  She makes use of the histogram of radiants, which is -- or hog descriptors, which is a well-known in computer vision.  And there's a lot of libraries that implement this for you if you're curious.  And then she through it at neural networks, and this turned out to be well, except for images that the cat image was head-on or close to head-on.  And with respect to data, there's a lot of cat data out there.  Like, a lot.  Too much.
      If you're interested in cat data, here's one cat database of images, cat faces, two gigabytes of cat faces for all of your cat face needs.  And, you know, it's, like, the output of some computer vision project.  It's kind of crazy what's out there.
      All right.  And lastly I'll quickly go over some resources that you might be interested in for folks that might want to play warned machine learning because it's not that scary, and there's a lot of resources out there to help you.
      So first off, like, where do I find data; right?  Well, like, there's a lot of data out there, but, unfortunately, there isn't, like, a single place to go find this data.  If your application has information, like, that seems to be the most relevant place to get your data.  Otherwise there are different public data sets, like, Amazon Web services has a public data set portal.  So krata could also be used if you're interested in open government information.  And there's a deposit other for UC Irvine, and a subreddit asking for data sets and sharing data sets.  So a lot of stuff out there.
      And additionally, like, you know, hosting your own end to end machine learning pipeline can be consuming and resource intensive.  So if this is something that's not your thing, like, you can kick it off to, like, one of many different, like, cloud or hosted solutions.  Like, Amazon and Microsoft both have pipelines for this.  And, you know, many different third party libraries, like, data or big ML or prediction IO.
      Here I note a few different NPM libraries that I found around different machine learning tasks.  I would say that, like, the these all have, like, different maturity levels, so I would, you know, pay attention to how well they performed for your specific task.  And lastly these are some of my favorite, like, APIs around intelligent tasks.  So, like, alchemy API is a full sweep professional grade, they have for things sentiment or entity extraction or other things related to natural language.  I think they do face detection and things like that.  Echoness is probably my favorite of all time, it's used for music intelligence and music discovery.  And wit AI is helpful, I've seen people use for entity abstraction or intent detection in text and speech.
      And I'll post these slides and the links on Twitter later.  But, yeah, thanks for listening, and I hope you learned a little bit.
      [clapping] 
      >> Wow that was great.
      >> Thank you.
      >> So you mentioned the bag of words, all I could think of is my mom's fat cat.
      >> Yeah, --
      >> And I'm channeling David Bowie right now in case you want to know why I have an eye patch on.
      >> I suppose you could make the burrito a part of ingredients.  Try it out.
      >> Yeah.  So I'm new to algorithms and computer science being a front-end developer.  What's a good -- what's a good easy algorithm to learn?
      >> For machine learning or just in general?
      >> Just in general.  Do you have any favorite -- because this is a advanced topic.
      >> Yeah.
      >> For people who are getting into algorithms, do you have any favorites.
      >> I don't have favorites.  I have things that you should learn different sorting algorithms like merge sort or binary search or something.  But machine learning, very easy to understand if you look it up.
      >> So sorting.
      >> Yeah.  Well, that's essentially what a mirror do.  They just work the data.
      >> Thank you so much.  Very important talk.
      >> Thank you.
      [clapping]
      >> So that was the last talk before the coffee break.  We want to give a shout out to some sponsors.  IBM, they have tables and would love for you to say, hi.  And there's a survey, you should have received an e-mail, but it's also at bit Lee/CascadiaFest.  And there's another thing that I'm blanking on.  Yeah, like that swag out on the tables is ready for you to grab.  So go get it.  We'll see you after the break.  
