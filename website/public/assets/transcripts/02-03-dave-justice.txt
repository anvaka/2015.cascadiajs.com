      >> Thank you, Helen.  That was wonderful.  Thank you for sharing the lovely story your grandparents.
      >> I know.  You guys know so much about my grandparents right now, which is kind of weird.  But I don't know.  You guys should all introduce yourselves to me so I'm, like, no, I know all those people.  It's cool they know my grandparents.
      >> Thank you very much.
      >> Yeah.  Thank you too.
      >> You're all set.
      >> Okay.  Cool.  I was, like, do I leave?
      >> I especially appreciate the context of Helen's presentation.  I love rooting things in the history of things finding out where they came from can always help us understand why they are the way they are today.  And I think that's a really fantastic viewpoint.
      Well, they're getting set up.  I want to introduce you to Dave justice.  Many of us know him by mean Dave.  Although he's probably the nicest guy I've ever encountered, so there you have it.
      Dave is an audio hacker out of Portland, and he's joining Mozilla soon, so that's awesome, Mozilla has all these rad people working for them.  And, yeah, he's going to talk about some sound related goodness that should probably knock your socks off.  Are you all set?
      >> Yeah.  I believe so.
      >> Welcome Dave.
      "Making Waves with DSP on the net."
      By: Dave Justice.
      >> So we're going to get started with a little definition first.  I want to describe what digit signal processing.  So our definition is the use of mathematics to modify an information signal.  A good way to understand this is to first go through some of the use cases why would you want to use some of this.  And I would like to break this into two basic categories.  One being forgetting data.  We're going to process our signal in order to get data, or we can modify our signal to actually produce a new signal or, you know, change the signal, the output from it.
      So some examples of getting data.  We can get the data about the signal itself.  So, for instance, we have, like, an audio signal we're going to get amplitude peaks from it, and maybe we'll visualize those or do something like that.  We can also get data about the environment through which the signal moves through.  An example of this is sonar and radar.
      The other side, the modifying of the signal, we have a couple of examples.  We might have output modified version of the signal itself.  So when we have a guitar plugs into a reverb effect pedal, we modify that signal, and that actual signal we're hearing the differences there.  We can produce new signals, we can synthesize a speech from a text signal, which is splitting off after processing and creating a whole separate signal.
      So we're going to go over terms here so that we can build up here.  We have a sign wave here.  Basically what we're doing is graphing a cyber wave across, and this is over time.  So we have our highest point here vertically we're going to be graphing amplitude.  So we're going to have positive numbers at the top, on the bottom, and it's just over time.
      So we see our peaks and our troughs.  Troughs are obviously going to be the lowest point.  Peeks are the top points.  I'm going to explain amplitude now.  As you see this is our sign wave once again graphed over this.  We'll see time increases as we go to the right.  And then up and down we still have also amplitude.  And it's going to show us basically our loudness, how we're going to perceive that.  We have a concept called magnitude, which we're going to see when we graph a different domain, how we graph frequency instead of time.  Basically a good cheating way to look at this, it's not the most accurate but a good way look at it is the amplitude and the positive values, and that's all we're going to show on the graph.
      And finally we get to frequency.  When we get to the frequency, we're going to be talking about the rate of repetition of events over time.  Now, how this is going to appear to us when we're talking about audio signals, it's going to sound like the pitch is changing.  As the frequency gets more rapid, it's going to go up higher and pitch for us.
      So this is kind of the basis of most digital processing, loo-loo of what we're going to focusing on today.  And Joseph, which we have a picture of here, came up with a crazy idea at the time, any wave form, this is any signal information can be basically broken down to the sum of a -- as many sign waves as you take to make it.  So we can start with one sign wave and continue adding to this to create this full signal.  This is really important for us.  So with digit signal processing, we're going to take that whole signal, tear it apart or build it.
      So we're going to explain the time domain now.  So there's two main domains that we're going to be talking about today.  The time domain and the frequency domain.  Time domain is very much what we showed before like the time waves.  This shows our examples though.  So this is really a bunch of sign waves mixed together basically.  It's a very basic example of it.  So this is our samples, graphed once again over time and with amplitude up here.
      We're going to get the frequency domain here.  So now what we see in this graph is we have our magnitude on the left side here, this is going to be our loudness also perceived.  All the way to the left we're going to have our lowest signals, so we're going to start around, like, 220 frequency, and then we'll rise up from there.  So our higher pitched frequencies are going to be all the way over on the right.
      So in order to switch between these two domains, so we can visualize them so we can operate, maybe there's thing we can operate in a frequency domain that's a little bit more efficient.  We are going to need the transform.  So the best way by looking at this is taking like that time examples into the pharmaceuticals.  And remember what we said before this is really just a bunch of sign waves making up this one wave form.
      So I have a visualization here that I've pulled from Wikipedia, it's the best visualization that I've seen from the FFT.  And as you can see here we have our main wave form, which is graphed over time and broken up into all these individual sign waves right here.  We're going to kind of set it to the side so we can visualize this.  And you can see the biggest sign wave right there it's the first one, the tall one, it's the loudest.  So now we have the frequency domain, as we increase in frequency, they get a little bit lower.  So this sound is going to be a little bit thicker, deeper base.  So we'll go through this again.
      This is the wave form over time, split it apart into sign waves, turn it on its side, and then we're going to be able to say our individual sign waves and then flipped over into the frequency graph.
      So all of this to tell you that we want to be able to use some of these things in the Web audio API.  I don't know if some of you May familiar with the Web audio API, it's been around want to a little bit now.  The most of things we've been using is now is mostly games and toy apps, which is unfortunate.  I would really like to see real work getting with it.  But we're getting to that point.  There's going to be a lot of updates.  So we're using like I said synthesizing audio, delayed nodes and these types of things and effects.  And then we're modifying those signals and scheduling sounds.  Scheduling is a lot of what we're doing in games.
      So if we want to go over real quick high level usage of this, we'll go ahead and step through a little function that we've written here.
      So we look here, and we have a function called frequency.  We're going to pass our frequency -- our function called play note, we're going to pass in frequency.  So first thing we need to do is get an audio context, and we get that straight off of window.  This is going to be our basis for what we build -- we get all our classes and things from.  So we get our audio context first.  You generally want to have one per session.  So, like, if you have a program that's going to be running on this, you only need one audio context.  No reason to work with more than one.  So first we're going to create our oscillator node.  That's going to be our source node, that's where we're going to get our signal from.  We're just going to start generating our signal from there.  We're going to create a gain node so that we can affect our loudness right there.  We're going to set our gain node all the way to one, which is this is between zero and one.  So we'll set it to one, that's, like, saying 100 percent.  We can set the wave shape to triangle for the oscillator node.  We're going to set the frequency to what we pass, which right here is 440, connect it up to the gain node, connect the gain node under which to the destination right.  So the destination I have not explained yet is basically just going to be our speakers.  It's going to be our speakers, headphones, anything like that.
      And then we're going to start our oscillator.  So probably the most confusing part about this API for me was trying to visualize this audio graph that we have to set up.  The audio graph is where we're detect each of these nodes.  Connecting the chain.  So to help visualize this thing.  Firefox released something a little wile ago that's the Web audio inspector; right?  So each time that we create these types of nodes, we have our oscillator, we see where it's connected to the gain and then connected to our insinuation, which is going to be our speakers here.  So I've come up with a little demo.  I went and took a project that I found, it's called streams editor, and on the train up here, I worked and forked it and made it work with our Web audio API here.  And I'm going to show you all this really quick so that we can -- oh, sick it doesn't work in Firefox.  I'm not going to show you all of that.
      Yeah, so wrote these slides in Chrome and then some things broke this morning, so I thought that Firefox was my savior this morning, but we'll come back to that.  So basically what we've done is created these nodes.  And we'll be sharing the link so you can play with these things.
      And we want to talk about our analyzer node.  It will be running like I said the discrete or fast transform over our signal allowing us to get data in the time or frequency domain.  What analyzer node actually does is goes in and samples all these signal in realtime, and it will give us in the time domain or it will run that transform over and give it to us in the frequency domain.  So let's run over the quick API for this really quick.  If you don't retain this that's, that's totally fine, these are going to be up and a lot of resources.  I just want to step through a basic understanding.
      And once again we've our oscillator node, we're going to create an analyzer node this time and a new thing that we haven't talked about yesterday.  We're going to get into the on the next side.  The script processor node.  And then we're going to sent up the empty array here, and we're going to set that to the frequency bin count, kind of arbitrary for these purposes.
      So we're going to connect or analyzer node up to our processor node, and we get this event hero, on audio process.  In that process, this is where we're ghoul be able to use our analyzer node.  In here we can get our byte frequency data.  It's a little bit different of a API, this was kind of a accusing thing for me.  Also when I gave this talk, it was confusing for people.  So this data, this array is so -- there's so many elements in it that we're not going to be able to use it the same way that we use most, like, API's in JavaScript where you just call the method, and it returns a copy of the array.  We need to pass in the array, and it's going to fill it up for us and right after that method call we're going to -- I don't know if you see the comment, but we're going to be able to loop all of this data and use the canvas, WebGL, all of these things.
      So this is run on audio process.  It's good to think of it as something that is going to run kind of, like, very similar to, like, request animation frame, something that's going to be called rapidly.  So you don't want to do crazy, like, you want to do crazy things in there, but you want to do it efficiently.
      And then we're going to do our analyzer node, we're going to connect it up to our oscillator, process node connect it up to the oscillator, and then we're going to start that.  And now when we set up our oscillator node, when we do that triangle wave, you see that triangle updating on canvas if I was rendering here.
      And we have an understanding of the node.  We'll go over the script processor node real quick.  We have that on audio processing event, and there's a couple downsides to it; right.  You have to write it so it's contained in your regular code.  It's hard to reuse.  It's not the best situation right now is basically what I'm getting at.
      So we want to do some examples of what I'm actually talking about when I want to visualize this stuff in 2D.  So we have our -- we're going to be visualizing the realtime -- the time domain in realtime.  We're going to be visualizing the frequency domain in realtime, and we're also going to graph out our entire wave form using the time domain data.
      So we have an example right over here.  So I'm going to go ahead and load some audio.  As you can see we're using that Web audio inspector right here so we can see our graph.  We have our destination -- we haven't loaded our sound yet, so it's going to change once we got this loaded in.  We have our destination, our analyzer, script processor, and then now that we're loaded in here, we have our source.  We have our gain node, and then we go over, split off, and we do our two analyzers, this is where we're going to do our drawing and I'm going to turn on the volume here.  Hopefully it's not too loud.
      Oh, sick.  I didn't plug it in.  So, yeah, I'm just going to turn this up really loud and hope y'all can hear it.  So basically we have our time domain here, this is our main wave, and then we have our frequency domain.  As you can see it's flipped over on its axis, and we have our lowest points of frequency right here and our highest over here.  And then finally we have our wave form.  And we're going to be tracking that progress across it S&L.  So this graphed the entire duration of the sample.
      Here we have our frequency domain and back in our time domain right here.  If any's interested, this sample is from a incredible composers, she was a Danish composer I believe from around the '40s.  She resisted in the World War II, and she's just an incredible person, you should definitely read about her.  Probably one of the most influential people from the electronic music from that early on.
      So, yeah, we'll switch over to this.  And we're going to go over a new thing.  This is what the really exciting part of this talk is.  So we have something coming called the audio worker node.  And so what this is we get to the create much like we have already a Web worker, but we get it with an audio context.  So we can clean up a lot of the weird code we have around this script processing things and hopefully make things we usable.  So like I said very similar to the Web worker, this runs in its own process, so we can be a little bit easier with, like, our processing.  It doesn't block our main thread; right?  And then we can create these custom nodes.  Hopefully we can, you know, just put them up on NPM and make them so everybody can use them.  But you have to write everything in your own file, it's hard to use, everyone's making their own abstractions around where this is a good enough abstraction where we can use it on its own I would say.
      So let's make a little Web worker ourselves.  Let's go to the API.  We have here -- let's do new, audio worker node, context, and pass through this string right here is a link to a file.  So we have our bit crusher worker right there.
      We're going to go ahead and set our bits to 8.  We're going to set frequency up here to .5.  And then in our separate file, we have our variables and our on audio method.  So we're going to just loop over all this data that we're going to pass in.  Pass in frequency data or our time data, and we're just going to be able to do all of our processing right here and spit it back out; right?
      This will be the type of effect where we'll actually modify that signal.  So, yeah, that's a good example of what we have here.  And we will talk to -- we'll communicate with this toot main thread the same way we do in workers currently by using messages passing.
      So that's an example.  There is a shim currently.  It's the audio worker node.  It was working great until this morning of course.  So not going to be able to show that to you, unfortunately.  I would like people to start using the shim as soon as it works again.  I'm going to try to make a request to it and make it work this afternoon.  Because I would like for people to start making these he needs to go, making these different visualizations in a very different reusable way and putting them up on NPM.  So we would like you to use audio workers, use this shim right now, there's no reason not to, except that it doesn't work.  There will be no reason not to, once it starts working.  Gosh.
      So, yeah, that's what I would like us to do.  I would like us to have a community effort here.  So I feel like there are has been a lot of good, very smart people working on these Web audio things.  But it's kind of scattered all over the place.  It's all over GitHub, it's all on just, like, giant files.  There are things on NPM, but it's hart to God them to work together.  So I've put together a little bit of a -- supposed to be helpful example here.  Which is going to be Web audio modules, I'll show you that in a minute.  And we're putting all of these together.  I don't know if you've seen a GitHub user Hugh SK, and he puts together a repo a little while ago called game montage pulls.  And it had all the small modules that you can find on NPM all in one place, and it's a good place to start because we all know there's discover ability on NPM, and they're getting better and better and these kinds of lists have been extremely helpful.  So I'm hoping they're helpful to y'all.  I still have a lot of things to add to it.  Right now there are a lot of my modules, I'm not trying to skew, so I'm going to be opening up a poll request.  We have our audio graph editor that I was attempting to show earlier.  Only use right now in Chrome.  And then we have a project called open music that's been some really, really neat things, especially, like, Web components that are wrapped around these as an distraction.  If you're interesting in talking to me about these, building these abstractions, any of these ideas that you have, please, like, bring them to me.  I'm in a channel called boring engineering is where I do all my Web audio constitute.  No one is in there but me, I just created a channel, so hopefully you join it and talk to me about it because I'm lonely.  We do have the main channel, Web audio where you can talk to Chris Wilson, one of the main people who are working on the Web audio spec.  He's always been very nice, helpful with these things.  Yeah, so thank you.  These are a couple of different things you might want to check out.  You can contact me, me Dave 2020 on Twitter, et cetera.  These are all the Web audio things.  So, yeah.  Thank you.
      [clapping] 
      >> Cool.  Thank you very much.  I think some of that went over my head a little bit.  But super cool stuff.
      >> Thanks.
      >> Is there any, like, specific genre of music you like doing the most?
      >> I like doing weird things.  A lot of times when I'm working on Web audio things, I end up listening to the same song over and over and over and I don't really hear it because I'll just be testing and refreshing it and it will drive my roommates insane.  So if I have ambient sounds, it's --
      >> Pretty considerate of you, though.
      >> Do you want to hang around while I do announcements?
      >> No.
      >> Okay.  Thanks so much.  We're going to have a little coffee break.  Before that, I have some announcements.  So first off, there is just a reminder about day care.  If you have not dropped off your kids in the day care I guess today seemed a little full, but you can go and check with them to see if they still have a little bit of room.  To, you know, watch your kids while you watch some talks.  You can also feel free to go in, see them, play with them at any point in the day.  You don't have to feel obligated to never see them again.  You can go and drop by at any point.  So that was just a reminder we wanted to share with you.
      We want to especially thank sponsors for the diversity scholarships, especially -- excuse me.  Just shout out these companies.  Microsoft, filament group, Travis foundation, craigslist, my employer, and individual donors who have brought people here today with the effort of -- or with the intention of furthering diversity, outreach, and I hope that all of those people have having a wonderful time.  We're really glad you're here.  That's fantastic.  Talks are going to start again at 11:20, so please be back for all of those and enjoy your coffee.  See you soon.
