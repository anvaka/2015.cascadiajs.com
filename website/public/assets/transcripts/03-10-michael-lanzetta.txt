      >> I've never been retweeted by horse.js, and I'm pretty sure no one who is actually horse.js is here because we haven't seen any tweets yet.
      So I think we're about ready to get started.  So our next speaker is Michael Lanzetta he's from Seattle, and he's going to tell us about containing the chaos.
      "Containing the Chaos: Building and Scaling IoT Node Services on Azure Using Paas and Containerization."
      By: Michael Lanzetta.
      >> Hey, all, I hope you can hear me.  Sounds like you can.  I realize it's the second-to-the-last talk of the day, so I tried to make any slides a nice, dark background so if you need to being O to take a nap, that's fine.  And I took another cup of coffee, so if my heart explodes, I apologize in advance.  And I'm here to tell you about a service we built in IOT and node and how it grew up into a larger service using containerization and platform as a service technologies.  And like I said I'm an open source engineer at Microsoft.  All of the stuff that we built is open source, it's all on GitHub.  So feel free to take it and run with it.  But IOT stands for the Internet of things.  And we were very excited about it because it is great, but it does have two minor problems.  The first is the Internet.  TCP is, you know, stateful, it's a connection technology, the problem is a lot of devices are flaky, so they'll drop connection commonly, and UDP is more for low powered flaky devices, but packets can be lost, they can be delivered out of order.  Security is necessary, but also expensive, and it takes a lot of DEV time and also expensive perfect flies.  That can be a problem.  Scaling up to the deal with the traffic you have on the Internet is also a problem.  There's a lot of messages coming in and handling that traffic can be a problem.  Servers go down.  They catch fire.  There's lots of wiring everywhere.  It can generally be a problem.  So that's one of the problems of the Internet of things.  The other is things.
      Every device that's out there has different levels of capabilities.  They have different memory footprints.  One of the devices we used is the TESSAL, and we were excited because it was running node, and the only problem is it was running node so Lodash wouldn't work and it wasn't functional for pus.
      A lot of devices have their own set of problems, they're low powered, so you might need a hub in your home, and that can be its own vector for attack, and the devices itself can be a vector for attack like you saw the fridge got a spam bottom.  That guy came and spoke at our office at one point, and it's freaky.  Don't ever let him talk to you because it scares the crap out of you.
      So in this presentation, I'll solve all of these problem.  Well, most of these prisons, well, actually only some.  It turns out most are difficult to solve and they're going to require IOT changes and IOT vendors, potently new protocols and a lot of education of both the builders and the consumers in the space.  So really I'm going to focus on the pieces that we do solve and leave the rest as an exercise to the reader.
      So nitrogen JS is our IOT solution and really it's our other IOT solution.  And it has a few core features.  It runs anywhere and everywhere.  The clouds there because it can run on any cloud.  It's not specific or as your specific even.  It will run on AWS, on your own home servers, whatever you want.  It supports a bunch of different common protocols, so it solves how your devices talk to the servers, so NQTT and a few other things.  It can write to a few other destinations, so it solves the question of how do your devices get data to where you can actually use it because it can write out to Mongo, to as are a tabling, so a bunch of different cues like event hub.  And it's interface run, so if it doesn't support what you want, it's relatively easy to add support.  And it deals with the core, the transport messaging, but it also deals with device identity, the registering of devices, the batching of device registry, so if you need to register, say, 10,000 devices and permission for both device and users.  So, hey, you know, I can see my fridge and figure out where it's going.
      And it's free as in beer and America.  It's fully IOS, and it's very measure missable license, and feel free to take it and sell it I guess.  I guess we're fine with that.
      So this is the nitrogen architecture, and this is really from the before times.  And I'll explain why I'm using an older architecture later in the talk.  But that's one of the motivators of the talk is how this architecture changed based on what happens.  And so if you look at how it was structured as you have these devices and they come in and suck to this registry and get provisioned and they can send information up to this service.  And then the applications that come in and, say, hey, I want to know what devices are in the system and the registry service will tell them the devices that they're allowed to see and they can ask for information about the devices.
      So applications can range from a bunch of different things.  From the consumers that write it out to Mongo or write it out to cues to user facing portals where you can actually see, hey, these are the three water heaters on my house or my fridge is still where I left it.  So we have this portal called OXI that we built, it's something you can use to built on top of if you want your own custom portals.  And it was built using ember, which he says is great, and he also thinking JavaScript is the best language out there.  So his judgment is impaired.  I still don't understand ember, but I believe him.
      So our use case that we're using is car telemetry, and the reason we're using is because we actually are using nitrogen internally for a lot of different things.  But car telemetry seemed like a good indicate because we have a lot of cars that are out there that we can use.  We -- that data can be used to a lot of different things.  It has geoinformation, it has time information, it has things that you can use for kind of predictive analytics scenarios.  And the other important thing is we had a client who wanted it so that was also useful.  I guess I'll let you keep on -- I love this video.
      So the basic deployment that we started with was we had a goal of monitoring a fleet of about 30,000 cars and each car would send up about two messages a second.  And the we expected about 37 percent peek current utilization of the fleet, which seems like an oddly specific number, so I'm guessing the client gave it to us, so it's probably a lie.
      So our employment was disguised to be capable of handling 60 percent utilization on the fleet on the off chance that that was a lie or that that needed to scale up.  So it worked out to about 60,000 messages a second for the car fleet as a whole.  And at the time each could handle around 1,000 messages a second.  Which worked out to about 60 servers for ingestion, 20 front doors for the devices and the front ends or the devices and the bash dashboards.  10 consumption instances, and five registry instances.  Mostly for fail over.  And it was about 100 instances total.  And even if you're dealing with VMs, it's a pain to manage 100 different servers.
      So, like, with any computer science problem, as Gandalf says, all problems can be solved in a layer of indirection.  At least I found that on the Internet, so I think it's Gandalf.  It's got to be true.  Basically it was kind of obvious that we needed another layer of indirection.
      So we went, and we turned to the various DEV op technology out there because that can help us avoid.  And we looked at a lot of the different ones that are out there, which we settled on these three, which kind of stack.  There's docker, core OS, and the way that this s docker kind of provides a level of indirection between you and the VM.  So instead of actually running a VM, what you're doing is running the things called containers.  And the idea behind it is you have your hoss -- in a typical VM scenario, you have your host and the hyper vicar layer and a bunch that are running on top of that and your applications are running on top of that.  And the issue there is that any time you want to deploy a new application or roll back because you deployed a stupid application, it takes a long time to do that roll back, and it takes a long time to fire these things up and down, and they're kind of expensive in terms of the amount of resources that they take.  But they do provide a good level of isolation between the applications.
      With containers, what you have as you have docker running on the host, and that actually basically just runs the different applications in containers on top of that.  As long as they're running the same -- like if they're all -- then have made all basically running into these container on that host, and it's very cheap to fire them up and tear them down.  So doing a new deployment is actually very fast using docker.  And the way that docker works is basically you take your application, and you package it up and send it up to a docker hub.  And this is the public docker hub, and basically you can see a bunch of applications are already there.  You can take them, use them out of the box, on extend them and use your own functionality on top of them and use that.  And there's also private docker hubs, which I'll get into a little bit later.  And this is what an application looks like in docker.  This is our Nginx docker file.  As you can see it starts and does a bunch of stuff.  Downloads a bunch of stuff, makes sure it's up to state, installs a few things, does some mods, and fires up the application.  So it's pretty simplistic, but it's a repeatable deployment that you can check into source control and makes the whole process relatively controlled.
      So containers are awesome.  And deploying stuff with containers was pretty awesome, and that's how we lived for a little while, and then we find out about core OS, which was an operating system that was built around the idea of containerization.  So what it is is basically a stripped down Linux, and it's optimized to run on these containers -- or it's optimized to run these containers I should say.  It's auto updating.  And basically what it does is it uses two kind of distributed technologies.  Fleet and SED to control how you push stuff out to the fleet.  So fleet talks to system deed to basically cause services to fire up and follow on the containers themselves.  Or on the machines themselves.  And SCD is kind of a distributed configuration system running raft is their consensus model.  And the idea is that SCD, basically you send stuff out to the fleet, it fires up, it registers, and then everything knows what's running.  And if something goes down, fleet fires out about it and fires up on a new host.  So it's built around the idea that these corresponds and containers and not the VMs.  And then there's a technology that people built on top of that once again with its layered scenario called day S, which builds on docker and core OS, and makes it disgustingly simple.
      So you basically do GIT push of new deployment, and that actually will take your can deployment, package it up with the configuration, into a new docker file.  Turn that into build that's registered in the docker hub registry, and then use that scheduler to flood that out to your fleet.  So you basically push out to your entire fleet into an entire sweet of apps.  And they all talk and say, hey, I'm running over here, and there's a low balance router layer on the front of this thing that basically says, oh, you're asking for this?  A client request comes in and says, oh, you're asking for this?  And directs it to the appropriate host.  So if you remember our previous infrastructure that we had, we had this idea of this kind of front door service that was brokering requests and then the ingestion and consumption and registry service that were down there.  And that front door service falls away with the day S router, and it routes to the machines appropriate to them.  So it routes to the registry machine or registry container and a messaging request gets routed to a messaging container.  And that's all seamless to you and up great and roll back are all relatively easy to do.  And you can figure out what's running on your machines with a simple day SPS and bring them up and county.
      So we've had this running for a couple of months in production, and it's worked out really well for us.  And I think we're going to continue it.  Did he have ops of course is a really kind of hot space right now.  So the entire technology stack will probably change by next year, which could be another talk.  And we'll find out what that looks like then.
      So that basically talks about the way that we scale nitrogen to handle the load coming in.  But once you actually get that data, what do you do with it from there?  And this is kind of where my talk pivots a little bit.  But basically the idea is that nitrogen is about safety and ingestion.  And actually processing that data is tough and takes more work and takes usually work by different teams.  So what we do is we kind of punt on that whole thing, and we pump all of that data into event hubs hubs because they scale way more than we needed and they can output to spark and storm pipeline and things like that.  And this is not an ad for event hubs.  I don't really care if you use them.  It's more an ad on the way I want to support them.
      They ignore us to ignore the downstream and it's a post request for us to post stuff to them, but that was too slow for the amount of data that was coming in.  Event hub, though, it supports this binary connection based protocol for talking to messaging servers.  But it only supports AMQPP10, which there were no existing libraries that would work for it.  There was no Cupid, which worked on Linux sometimes, but it wasn't really a proper node library and definitely didn't deploy on devices because it wouldn't work on proton and it was a gyp to get installed.
      So I decided the easiest thing to do was to build my own, which is an incredible stupid thing to do.  In fact, the person who wrote the library told me not to do this.  And I was, like, it's 09 to 1O, which could be the problem?  Turns out it's a complete rewrite of the Protestant toll.  There are four things wrong, and that's AMQ and B.  So what I did find out is that writing bay nary protocols in node is really fun.  Especially ones that deal with things like 64 by the integers, which JavaScript doesn't have, or bit twiddling, and then keeping track of buffers was a nightmare.  And I would like to give a shout out to whoever where to the BL package on NPM, it's super useful, it's the loudest and made this a lot easier.  And then for the two over version library, we're going with a fully permissible library, and then in the face of failures is kind of new to me, I'm not sure how to do it.  So what we're doing is auto kind of venting in that case.  I'm still not sure that that's quite right.
      Debugging binary protocols.  You he had been up looking at a lot of scrolls that look like that.  The bottom part is just a bunch of hex and you figure out how to hex it.  I also don't recommend doing that.  There's a program out there called wire shark that helps out with things like that.
      And this is why this library is great.  It has no native code dependencies, societies just pure code, which because he needed we needed to run on really difficult devices these small machines that would only support node and not necessarily strange C libraries.  And we needed to run are you in on more devices like Windows boxes.  And this basically allowed us to do that pretty easily.  We're using bluebird and then hopefully eventually ESX.  And the we have a policy base configuration and the idea is you can take these I would say 30,000 different options for connecting to a AMQ pServer and have standard policies for the different types of servers and then just override them where you need to.  So you can kind of use it dumb or you can go deep and go, hey, I want to flow control like crazy, and I want to do really strange things.  And you can do that with just minor overrides with these policy classes.
      So we want you nitrogen right now it supports MQTT, Mongo, it should support more.  So if you wanted to help us add things, that would be fact.  It supports could you be I had D, rapped MQ, more testing would be great.  Even if you want to just use the systems and complain about them, that's totally fine about them.  Just complain to me or in the issues page.  There's still more work to do, and we would love feedback.
      So some of you may be asking why we didn't use the IOT sweet.  And others asking what is the IOT sweetest, and there maybe a few of you asking what is AJER.  The reason we didn't use it when starting out.  There were two reasons.  One that we didn't something that was agnostic to AJER that would run on a variety of platforms and would drive if you wanted to use that into processing.  And the slighter more important is that it it didn't exist.  And the reason we're not using it now is because it still doesn't kind of exist.  It's mostly there but not quite.  And so what we're doing is we have a close relationship with the AJERIOT team and are responsible for basically giving them feedback as they go, and we've made sure that, for instance, they're prioritizing their node libraries over other libraries for other languages.  Because we know that that's kind of the future of this whole space and, you know, as it makes sense to incorporate their stuff, we will.  So I do think they have things to offer.  But nitrogen will run anywhere, so that's one of the reasons we like it.
      So that's really the gist of my talk.  If you want more information, the links here to the main site to GitHub for both nitrogen and the library and to these slides.  And down below as you see AKMS Cascadia, that's actually the link to the slides.  You can find all these links on there.  And like I said I'm a Microsoft open source engineer, which when I actually started on this team was a rare bird, and now that they've open sourced, it's a little bit more common.  And there's a pols on what it means to be a Microsoft open source engineer.  My blog, my GitHub, and my Twitter if you want to get in contact with me, even if it's just to send me hate mail, that's totally fine.  Anyway I would like to thank you all for listening to me and have a good rest of the conference.
